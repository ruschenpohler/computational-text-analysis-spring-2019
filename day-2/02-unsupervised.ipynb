{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised methods\n",
    "\n",
    "In this lesson, we'll cover unsupervised computational text anlalysis approaches. The central methods covered are TF-IDF and Topic Modeling. Both of these are common approachs in the social sciences and humanities.\n",
    "\n",
    "[DTM/TF-IDF](#dtm)<br>\n",
    "\n",
    "[Topic modeling](#topics)<br>\n",
    "\n",
    "### Today you will\n",
    "* Understand the DTM and why it's important to text analysis\n",
    "* Learn how to create a DTM in Python\n",
    "* Learn basic functionality of Python's package scikit-learn\n",
    "* Understand tf-idf scores\n",
    "* Learn a simple way to identify distinctive words\n",
    "* Implement a basic topic modeling algorithm and learn how to tweak it\n",
    "* In the process, gain more familiarity and comfort with the Pandas package and manipulating data\n",
    "\n",
    "\n",
    "### Key Jargon\n",
    "* *Document Term Matrix*:\n",
    "    * a matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms.\n",
    "* *TF-IDF Scores*: \n",
    "    * short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "* *Topic Modeling*:\n",
    "    * A general class of statistical models that uncover abstract topics within a text. It uses the co-occurrence of words within documents, compared to their distribution across documents, to uncover these abstract themes. The output is a list of weighted words, which indicate the subject of each topic, and a weight distribution across topics for each document.\n",
    "    \n",
    "* *LDA*:\n",
    "    * Latent Dirichlet Allocation. A particular model for topic modeling. It does not take document order into account, unlike other topic modeling algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTM/TF-IDF <a id='dtm'></a>\n",
    "\n",
    "In this lesson we will use Python's scikit-learn package learn to make a document term matrix from a .csv Music Reviews dataset (collected from MetaCritic.com). We will then use the DTM and a word weighting technique called tf-idf (term frequency inverse document frequency) to identify important and discriminating words within this dataset (utilizing the Pandas package). The illustrating question: **what words distinguish reviews of Rap albums, Indie Rock albums, and Jazz albums?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "music_fname = 'music_reviews.csv'\n",
    "music_fname = os.path.join(DATA_DIR, music_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First attempt at reading in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>release_date</th>\n",
       "      <th>critic</th>\n",
       "      <th>score</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't Panic</td>\n",
       "      <td>All Time Low</td>\n",
       "      <td>Pop/Rock</td>\n",
       "      <td>2012-10-09 00:00:00</td>\n",
       "      <td>Kerrang!</td>\n",
       "      <td>74.0</td>\n",
       "      <td>While For Baltimore proves they can still writ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fear and Saturday Night</td>\n",
       "      <td>Ryan Bingham</td>\n",
       "      <td>Country</td>\n",
       "      <td>2015-01-20 00:00:00</td>\n",
       "      <td>Uncut</td>\n",
       "      <td>70.0</td>\n",
       "      <td>There's nothing fake about the purgatorial nar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Way I'm Livin'</td>\n",
       "      <td>Lee Ann Womack</td>\n",
       "      <td>Country</td>\n",
       "      <td>2014-09-23 00:00:00</td>\n",
       "      <td>Q Magazine</td>\n",
       "      <td>84.0</td>\n",
       "      <td>All life's disastrous lows are here on a caree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Doris</td>\n",
       "      <td>Earl Sweatshirt</td>\n",
       "      <td>Rap</td>\n",
       "      <td>2013-08-20 00:00:00</td>\n",
       "      <td>Pitchfork</td>\n",
       "      <td>82.0</td>\n",
       "      <td>With Doris, Odd Future’s Odysseus is finally b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Giraffe</td>\n",
       "      <td>Echoboy</td>\n",
       "      <td>Rock</td>\n",
       "      <td>2003-02-25 00:00:00</td>\n",
       "      <td>AllMusic</td>\n",
       "      <td>71.0</td>\n",
       "      <td>Though Giraffe is definitely Echoboy's most im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     album           artist     genre         release_date  \\\n",
       "0              Don't Panic     All Time Low  Pop/Rock  2012-10-09 00:00:00   \n",
       "1  Fear and Saturday Night     Ryan Bingham   Country  2015-01-20 00:00:00   \n",
       "2       The Way I'm Livin'   Lee Ann Womack   Country  2014-09-23 00:00:00   \n",
       "3                    Doris  Earl Sweatshirt       Rap  2013-08-20 00:00:00   \n",
       "4                  Giraffe          Echoboy      Rock  2003-02-25 00:00:00   \n",
       "\n",
       "       critic  score                                               body  \n",
       "0    Kerrang!   74.0  While For Baltimore proves they can still writ...  \n",
       "1       Uncut   70.0  There's nothing fake about the purgatorial nar...  \n",
       "2  Q Magazine   84.0  All life's disastrous lows are here on a caree...  \n",
       "3   Pitchfork   82.0  With Doris, Odd Future’s Odysseus is finally b...  \n",
       "4    AllMusic   71.0  Though Giraffe is definitely Echoboy's most im...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv(music_fname, sep='\\t')\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the text of the first review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While For Baltimore proves they can still write a grade A banger when they put their mind to it, too many songs are destined to have \"must try harder\" stamped on their report card. [13 Oct 2012, p.52]\n"
     ]
    }
   ],
   "source": [
    "print(reviews['body'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Data using Pandas\n",
    "\n",
    "Let's first look at some descriptive statistics about this dataset, to get a feel for what's in it. We'll do this using the Pandas package. \n",
    "\n",
    "Note: this is always good practice. It serves two purposes. It checks to make sure your data is correct, and there's no major errors. It also keeps you in touch with your data, which will help with interpretation. <3 your data!\n",
    "\n",
    "First, what genres are in this dataset, and how many reviews in each genre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pop/Rock                  1486\n",
       "Indie                     1115\n",
       "Rock                       932\n",
       "Electronic                 513\n",
       "Rap                        363\n",
       "Pop                        149\n",
       "Country                    140\n",
       "R&B;                       112\n",
       "Folk                        70\n",
       "Alternative/Indie Rock      42\n",
       "Dance                       41\n",
       "Jazz                        38\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can count this using the value_counts() function\n",
    "reviews['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing most people do is to `describe` their data. (This is the `summary` command in R, or the `sum` command in Stata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5001.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>72.684223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.714896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>68.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>74.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>79.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             score\n",
       "count  5001.000000\n",
       "mean     72.684223\n",
       "std       8.714896\n",
       "min       7.400000\n",
       "25%      68.000000\n",
       "50%      74.000000\n",
       "75%      79.000000\n",
       "max     100.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There's only one numeric column in our data so we only get one column for output.\n",
    "reviews.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only gets us numerical summaries. To get summaries of some of the other columns, we can explicitly ask for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>release_date</th>\n",
       "      <th>critic</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5001</td>\n",
       "      <td>5001</td>\n",
       "      <td>5001</td>\n",
       "      <td>5001</td>\n",
       "      <td>5001</td>\n",
       "      <td>5001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3799</td>\n",
       "      <td>2607</td>\n",
       "      <td>12</td>\n",
       "      <td>956</td>\n",
       "      <td>592</td>\n",
       "      <td>4998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Nobody's Daughter</td>\n",
       "      <td>Various Artists</td>\n",
       "      <td>Pop/Rock</td>\n",
       "      <td>2011-09-13 00:00:00</td>\n",
       "      <td>AllMusic</td>\n",
       "      <td>He does express regret about the marriage brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>1486</td>\n",
       "      <td>29</td>\n",
       "      <td>282</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    album           artist     genre         release_date  \\\n",
       "count                5001             5001      5001                 5001   \n",
       "unique               3799             2607        12                  956   \n",
       "top     Nobody's Daughter  Various Artists  Pop/Rock  2011-09-13 00:00:00   \n",
       "freq                    5               22      1486                   29   \n",
       "\n",
       "          critic                                               body  \n",
       "count       5001                                               5001  \n",
       "unique       592                                               4998  \n",
       "top     AllMusic  He does express regret about the marriage brea...  \n",
       "freq         282                                                  2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who were the reviewers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AllMusic                     282\n",
       "PopMatters                   228\n",
       "Pitchfork                    207\n",
       "Q Magazine                   178\n",
       "Uncut                        171\n",
       "Mojo                         137\n",
       "Drowned In Sound             132\n",
       "New Musical Express (NME)    127\n",
       "The A.V. Club                121\n",
       "Rolling Stone                112\n",
       "Name: critic, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['critic'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the artists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Various Artists      22\n",
       "R.E.M.               16\n",
       "Arcade Fire          14\n",
       "Sigur Rós            13\n",
       "Belle & Sebastian    12\n",
       "Brian Eno            11\n",
       "Low                  10\n",
       "Weezer               10\n",
       "The Raveonettes      10\n",
       "Kings of Leon        10\n",
       "Name: artist, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['artist'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the average score as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.68422315536893"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to know the average score for each genre? To do this, we use Pandas `groupby` function. You'll want to get very familiar with the `groupby` function. It's quite powerful. (Similar to `collapse` on Stata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "Jazz                      77.631579\n",
       "Folk                      75.900000\n",
       "Indie                     74.400897\n",
       "Country                   74.071429\n",
       "Alternative/Indie Rock    73.928571\n",
       "Electronic                73.140351\n",
       "Pop/Rock                  73.033782\n",
       "R&B;                      72.366071\n",
       "Rap                       72.173554\n",
       "Rock                      70.754292\n",
       "Dance                     70.146341\n",
       "Pop                       64.608054\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_grouped_by_genre = reviews.groupby(\"genre\")\n",
    "reviews_grouped_by_genre['score'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the DTM using scikit-learn\n",
    "\n",
    "Ok, that's the summary of the metadata. Next, we turn to analyzing the text of the reviews. Remember, the text is stored in the 'body' column. First, a preprocessing step to remove numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>release_date</th>\n",
       "      <th>critic</th>\n",
       "      <th>score</th>\n",
       "      <th>body</th>\n",
       "      <th>body_without_digits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't Panic</td>\n",
       "      <td>All Time Low</td>\n",
       "      <td>Pop/Rock</td>\n",
       "      <td>2012-10-09 00:00:00</td>\n",
       "      <td>Kerrang!</td>\n",
       "      <td>74.0</td>\n",
       "      <td>While For Baltimore proves they can still writ...</td>\n",
       "      <td>While For Baltimore proves they can still writ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fear and Saturday Night</td>\n",
       "      <td>Ryan Bingham</td>\n",
       "      <td>Country</td>\n",
       "      <td>2015-01-20 00:00:00</td>\n",
       "      <td>Uncut</td>\n",
       "      <td>70.0</td>\n",
       "      <td>There's nothing fake about the purgatorial nar...</td>\n",
       "      <td>There's nothing fake about the purgatorial nar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Way I'm Livin'</td>\n",
       "      <td>Lee Ann Womack</td>\n",
       "      <td>Country</td>\n",
       "      <td>2014-09-23 00:00:00</td>\n",
       "      <td>Q Magazine</td>\n",
       "      <td>84.0</td>\n",
       "      <td>All life's disastrous lows are here on a caree...</td>\n",
       "      <td>All life's disastrous lows are here on a caree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Doris</td>\n",
       "      <td>Earl Sweatshirt</td>\n",
       "      <td>Rap</td>\n",
       "      <td>2013-08-20 00:00:00</td>\n",
       "      <td>Pitchfork</td>\n",
       "      <td>82.0</td>\n",
       "      <td>With Doris, Odd Future’s Odysseus is finally b...</td>\n",
       "      <td>With Doris, Odd Future’s Odysseus is finally b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Giraffe</td>\n",
       "      <td>Echoboy</td>\n",
       "      <td>Rock</td>\n",
       "      <td>2003-02-25 00:00:00</td>\n",
       "      <td>AllMusic</td>\n",
       "      <td>71.0</td>\n",
       "      <td>Though Giraffe is definitely Echoboy's most im...</td>\n",
       "      <td>Though Giraffe is definitely Echoboy's most im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Outer South</td>\n",
       "      <td>Conor Oberst And The Mystic Valley Band</td>\n",
       "      <td>Indie</td>\n",
       "      <td>2009-05-05 00:00:00</td>\n",
       "      <td>Slant Magazine</td>\n",
       "      <td>67.0</td>\n",
       "      <td>The result is an album that's unfortunately ba...</td>\n",
       "      <td>The result is an album that's unfortunately ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>On An Island</td>\n",
       "      <td>David Gilmour</td>\n",
       "      <td>Rock</td>\n",
       "      <td>2006-03-07 00:00:00</td>\n",
       "      <td>E! Online</td>\n",
       "      <td>67.0</td>\n",
       "      <td>In the end, Island makes Dave sound like he's ...</td>\n",
       "      <td>In the end, Island makes Dave sound like he's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Movement</td>\n",
       "      <td>Gossip</td>\n",
       "      <td>Indie</td>\n",
       "      <td>2003-05-06 00:00:00</td>\n",
       "      <td>Uncut</td>\n",
       "      <td>81.0</td>\n",
       "      <td>Beth Ditto's remarkable gospel holler and ferv...</td>\n",
       "      <td>Beth Ditto's remarkable gospel holler and ferv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Locked Down</td>\n",
       "      <td>Dr. John</td>\n",
       "      <td>Pop/Rock</td>\n",
       "      <td>2012-04-03 00:00:00</td>\n",
       "      <td>PopMatters</td>\n",
       "      <td>86.0</td>\n",
       "      <td>Dr. John is Dr. John. He's a star, and is on f...</td>\n",
       "      <td>Dr. John is Dr. John. He's a star, and is on f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>And Their Refinement Of The Decline</td>\n",
       "      <td>Stars Of The Lid</td>\n",
       "      <td>Rock</td>\n",
       "      <td>2007-04-07 00:00:00</td>\n",
       "      <td>PopMatters</td>\n",
       "      <td>87.0</td>\n",
       "      <td>Their work, especially that displayed on Refin...</td>\n",
       "      <td>Their work, especially that displayed on Refin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5001 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    album  \\\n",
       "0                             Don't Panic   \n",
       "1                 Fear and Saturday Night   \n",
       "2                      The Way I'm Livin'   \n",
       "3                                   Doris   \n",
       "4                                 Giraffe   \n",
       "...                                   ...   \n",
       "4996                          Outer South   \n",
       "4997                         On An Island   \n",
       "4998                             Movement   \n",
       "4999                          Locked Down   \n",
       "5000  And Their Refinement Of The Decline   \n",
       "\n",
       "                                       artist     genre         release_date  \\\n",
       "0                                All Time Low  Pop/Rock  2012-10-09 00:00:00   \n",
       "1                                Ryan Bingham   Country  2015-01-20 00:00:00   \n",
       "2                              Lee Ann Womack   Country  2014-09-23 00:00:00   \n",
       "3                             Earl Sweatshirt       Rap  2013-08-20 00:00:00   \n",
       "4                                     Echoboy      Rock  2003-02-25 00:00:00   \n",
       "...                                       ...       ...                  ...   \n",
       "4996  Conor Oberst And The Mystic Valley Band     Indie  2009-05-05 00:00:00   \n",
       "4997                            David Gilmour      Rock  2006-03-07 00:00:00   \n",
       "4998                                   Gossip     Indie  2003-05-06 00:00:00   \n",
       "4999                                 Dr. John  Pop/Rock  2012-04-03 00:00:00   \n",
       "5000                         Stars Of The Lid      Rock  2007-04-07 00:00:00   \n",
       "\n",
       "              critic  score  \\\n",
       "0           Kerrang!   74.0   \n",
       "1              Uncut   70.0   \n",
       "2         Q Magazine   84.0   \n",
       "3          Pitchfork   82.0   \n",
       "4           AllMusic   71.0   \n",
       "...              ...    ...   \n",
       "4996  Slant Magazine   67.0   \n",
       "4997       E! Online   67.0   \n",
       "4998           Uncut   81.0   \n",
       "4999      PopMatters   86.0   \n",
       "5000      PopMatters   87.0   \n",
       "\n",
       "                                                   body  \\\n",
       "0     While For Baltimore proves they can still writ...   \n",
       "1     There's nothing fake about the purgatorial nar...   \n",
       "2     All life's disastrous lows are here on a caree...   \n",
       "3     With Doris, Odd Future’s Odysseus is finally b...   \n",
       "4     Though Giraffe is definitely Echoboy's most im...   \n",
       "...                                                 ...   \n",
       "4996  The result is an album that's unfortunately ba...   \n",
       "4997  In the end, Island makes Dave sound like he's ...   \n",
       "4998  Beth Ditto's remarkable gospel holler and ferv...   \n",
       "4999  Dr. John is Dr. John. He's a star, and is on f...   \n",
       "5000  Their work, especially that displayed on Refin...   \n",
       "\n",
       "                                    body_without_digits  \n",
       "0     While For Baltimore proves they can still writ...  \n",
       "1     There's nothing fake about the purgatorial nar...  \n",
       "2     All life's disastrous lows are here on a caree...  \n",
       "3     With Doris, Odd Future’s Odysseus is finally b...  \n",
       "4     Though Giraffe is definitely Echoboy's most im...  \n",
       "...                                                 ...  \n",
       "4996  The result is an album that's unfortunately ba...  \n",
       "4997  In the end, Island makes Dave sound like he's ...  \n",
       "4998  Beth Ditto's remarkable gospel holler and ferv...  \n",
       "4999  Dr. John is Dr. John. He's a star, and is on f...  \n",
       "5000  Their work, especially that displayed on Refin...  \n",
       "\n",
       "[5001 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_digits(comment):\n",
    "    return ''.join([ch for ch in comment if not ch.isdigit()])\n",
    "\n",
    "reviews['body_without_digits'] = reviews['body'].apply(remove_digits)\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    While For Baltimore proves they can still writ...\n",
       "1    There's nothing fake about the purgatorial nar...\n",
       "2    All life's disastrous lows are here on a caree...\n",
       "3    With Doris, Odd Future’s Odysseus is finally b...\n",
       "4    Though Giraffe is definitely Echoboy's most im...\n",
       "Name: body_without_digits, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['body_without_digits'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer Function\n",
    "\n",
    "Our next step is to turn the text into a document term matrix using the scikit-learn function called `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvec = CountVectorizer()\n",
    "sparse_dtm = countvec.fit_transform(reviews['body_without_digits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We made a DTM! Let's look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5001x16139 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 124340 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is called Compressed Sparse Format. It save a lot of memory to store the dtm in this format, but it is difficult to look at for a human. To illustrate the techniques in this lesson we will first convert this matrix back to a Pandas DataFrame, a format we're more familiar with. For larger datasets, you will have to use the Compressed Sparse Format. Putting it into a DataFrame, however, will enable us to get more comfortable with Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aahs</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abc</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zooey</th>\n",
       "      <th>zoomer</th>\n",
       "      <th>zu</th>\n",
       "      <th>zydeco</th>\n",
       "      <th>álbum</th>\n",
       "      <th>être</th>\n",
       "      <th>über</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaaa  aahs  aaliyah  aaron  ab  abandon  abandoned  abandoning  abc  \\\n",
       "0   0     0     0        0      0   0        0          0           0    0   \n",
       "1   0     0     0        0      0   0        0          0           0    0   \n",
       "2   0     0     0        0      0   0        0          0           0    0   \n",
       "3   0     0     0        0      0   0        0          0           0    0   \n",
       "4   0     0     0        0      0   0        0          0           0    0   \n",
       "\n",
       "   ...  zone  zones  zoo  zooey  zoomer  zu  zydeco  álbum  être  über  \n",
       "0  ...     0      0    0      0       0   0       0      0     0     0  \n",
       "1  ...     0      0    0      0       0   0       0      0     0     0  \n",
       "2  ...     0      0    0      0       0   0       0      0     0     0  \n",
       "3  ...     0      0    0      0       0   0       0      0     0     0  \n",
       "4  ...     0      0    0      0       0   0       0      0     0     0  \n",
       "\n",
       "[5 rows x 16139 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm = pd.DataFrame(sparse_dtm.toarray(), columns=countvec.get_feature_names(), index=reviews.index)\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can we do with a DTM?\n",
    "\n",
    "We can quickly identify the most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the      7406\n",
       "and      4557\n",
       "of       4400\n",
       "to       3175\n",
       "is       2914\n",
       "it       2608\n",
       "that     2039\n",
       "in       1775\n",
       "album    1719\n",
       "this     1518\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "* Print out the most infrequent words rather than the most frequent words. You can look at the [Pandas documentation](http://pandas.pydata.org/pandas-docs/stable/api.html#api-dataframe-stats) for more information.\n",
    "* Print the average number of times each word is used in a review.\n",
    "* Print this out sorted from highest to lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF scores\n",
    "\n",
    "\n",
    "How to find distinctive words in a corpus is a long-standing question in text analysis. Today, we'll learn one simple approach to this: TF-IDF. The idea behind words scores is to weight words not just by their frequency, but by their frequency in one document compared to their distribution across all documents. Words that are frequent, but are also used in every single document, will not be distinguising. We want to identify words that are unevenly distributed across the corpus.\n",
    "\n",
    "One of the most popular ways to weight words (beyond frequency counts) is `tf-idf score`. By offsetting the frequency of a word by its document frequency (the number of documents in which it appears) will in theory filter out common terms such as 'the', 'of', and 'and'.\n",
    "\n",
    "Traditionally, the *inverse document frequency* of word $j$ is calculated as:\n",
    "\n",
    "$idf_{j} = log\\left(\\frac{\\#docs}{\\#docs\\,with\\,j}\\right)$ \n",
    "\n",
    "and the *term freqency - inverse document frequency* is \n",
    "\n",
    "$tfidf_{ij} = f_{ij}\\times{idf_j}$ where $f_{ij}$ is the number of occurences of word $j$ in document $i$.\n",
    "\n",
    "You can, and often should, normalize the word frequency: \n",
    "\n",
    "$tfidf_{ij} = \\frac{f_{ij}}{\\#words\\,in\\,doc\\,i}\\times{idf_{j}}$\n",
    "\n",
    "We can calculate this manually, but scikit-learn has a built-in function to do so. This function also uses log frequencies, so the numbers will not correspond excactly to the calculations above. We'll use the [scikit-learn calculation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), but a challenge for you: use Pandas to calculate this manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDFVectorizer Function\n",
    "\n",
    "To do so, we simply do the same thing we did above with CountVectorizer, but instead we use the function TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5001x16139 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 124340 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfvec = TfidfVectorizer()\n",
    "sparse_tfidf = tfidfvec.fit_transform(reviews['body_without_digits'])\n",
    "sparse_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aahs</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abc</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zooey</th>\n",
       "      <th>zoomer</th>\n",
       "      <th>zu</th>\n",
       "      <th>zydeco</th>\n",
       "      <th>álbum</th>\n",
       "      <th>être</th>\n",
       "      <th>über</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaaa  aahs  aaliyah  aaron   ab  abandon  abandoned  abandoning  abc  \\\n",
       "0  0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0  0.0   \n",
       "1  0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0  0.0   \n",
       "2  0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0  0.0   \n",
       "3  0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0  0.0   \n",
       "4  0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0  0.0   \n",
       "\n",
       "   ...  zone  zones  zoo  zooey  zoomer   zu  zydeco  álbum  être  über  \n",
       "0  ...   0.0    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
       "1  ...   0.0    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
       "2  ...   0.0    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
       "3  ...   0.0    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
       "4  ...   0.0    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 16139 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = pd.DataFrame(sparse_tfidf.toarray(), columns=tfidfvec.get_feature_names(), index=reviews.index)\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the 20 words with highest tf-idf weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brill         1.000000\n",
       "perfect       1.000000\n",
       "yummy         1.000000\n",
       "pppperfect    1.000000\n",
       "awesome       1.000000\n",
       "wonderfull    1.000000\n",
       "meh           1.000000\n",
       "stars         1.000000\n",
       "subpar        0.959257\n",
       "ga            0.908259\n",
       "masterful     0.898620\n",
       "grower        0.888624\n",
       "likable       0.867803\n",
       "acirc         0.867003\n",
       "great         0.864253\n",
       "infectious    0.859996\n",
       "blank         0.854475\n",
       "thrilling     0.848810\n",
       "smart         0.847852\n",
       "stuff         0.834479\n",
       "dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.max().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! We have successfully identified content words, without removing stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Distinctive Words\n",
    "\n",
    "What can we do with this? These scores are best used when you want to identify distinctive words for individual documents, or groups of documents, compared to other groups or the corpus as a whole. To illustrate this, let's compare three genres and identify the most distinctive words by genre.\n",
    "\n",
    "First we add in a column of genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aahs</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abc</th>\n",
       "      <th>...</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zooey</th>\n",
       "      <th>zoomer</th>\n",
       "      <th>zu</th>\n",
       "      <th>zydeco</th>\n",
       "      <th>álbum</th>\n",
       "      <th>être</th>\n",
       "      <th>über</th>\n",
       "      <th>genre_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Pop/Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaaa  aahs  aaliyah  aaron   ab  abandon  abandoned  abandoning  abc  \\\n",
       "0  0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0  0.0   \n",
       "1  0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0  0.0   \n",
       "2  0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0  0.0   \n",
       "3  0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0  0.0   \n",
       "4  0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0  0.0   \n",
       "\n",
       "   ...  zones  zoo  zooey  zoomer   zu  zydeco  álbum  être  über    genre_  \n",
       "0  ...    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  Pop/Rock  \n",
       "1  ...    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0   Country  \n",
       "2  ...    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0   Country  \n",
       "3  ...    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0       Rap  \n",
       "4  ...    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0      Rock  \n",
       "\n",
       "[5 rows x 16140 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf['genre_'] = reviews['genre']\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare the words with the highest tf-idf weight for each genre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blank        0.854475\n",
       "waste        0.755918\n",
       "amiable      0.730963\n",
       "awesomely    0.717079\n",
       "joyless      0.687687\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rap = tfidf[tfidf['genre_']=='Rap']\n",
    "indie = tfidf[tfidf['genre_']=='Indie']\n",
    "jazz = tfidf[tfidf['genre_']=='Jazz']\n",
    "\n",
    "rap.max(numeric_only=True).sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "meh           1.0\n",
       "awesome       1.0\n",
       "wonderfull    1.0\n",
       "perfect       1.0\n",
       "yummy         1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indie.max(numeric_only=True).sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "purely        0.544477\n",
       "descending    0.519218\n",
       "devotional    0.507724\n",
       "recordings    0.499963\n",
       "languid       0.487715\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jazz.max(numeric_only=True).sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! A method of identifying distinctive words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "\n",
    "Instead of outputting the highest weighted words, output the lowest weighted words. How should we interpret these words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aa             0.0\n",
       "potent         0.0\n",
       "potential      0.0\n",
       "potentially    0.0\n",
       "potion         0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jazz.max(numeric_only=True).sort_values().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling <a id='topics'></a>\n",
    "\n",
    "The goal of topic models can be twofold: 1/ learning something about the topics themselves, i.e what the the ext is about 2/ reduce the dimensionality of text to represent a document as a weighted average of K topics instead of a vector of token counts over the whole vocabulary. In the latter case, topic modeling a way to treat text as any data in a more tractable way for any subsequent statistical analysis (linear/logistic regression, etc). \n",
    "\n",
    "There are many topic modeling algorithms, but we'll use LDA. This is a standard model to use. Again, the goal is not to learn everything you need to know about topic modeling. Instead, this will provide you some starter code to run a simple model, with the idea that you can use this base of knowledge to explore this further.\n",
    "\n",
    "We will run Latent Dirichlet Allocation, the most basic and the oldest version of topic modeling$^1$. We will run this in one big chunk of code. Our challenge: use our knowledge of scikit-learn that we gained above to walk through the code to understand what it is doing. Your challenge: figure out how to modify this code to work on your own data, and/or tweak the parameters to get better output.\n",
    "\n",
    "First, a bit of theory. LDA is a generative model - a model over the entire data generating process - in which a document is a mixture of topics and topics are probability distributions over tokens in the vocabulary. The (normalized) frequency of word $j$ in document $i$ can be written as:\n",
    "$q_{ij} = v_{i1}*\\theta_{1j} + v_{i2}*\\theta_{2j} + ... + v_{iK}*\\theta_{Kj}$\n",
    "where K is the total number of topics, $\\theta_{kj}$ is the probability that word $j$ shows up in topic $k$ and $v_{ik}$ is the weight assigned to topic $k$ in document $i$. The model treats $v$ and $\\theta$ as generated from Dirichlet-distributed priors and can be estimated through Maximum Likelihood or Bayesian methods.\n",
    "\n",
    "Note: we will be using a different dataset for this technique. The music reviews in the above dataset are often short, one word or one sentence reviews. Topic modeling is not really appropriate for texts that are this short. Instead, we want texts that are longer and are composed of multiple topics each. For this exercise we will use a database of children's literature from the 19th century. \n",
    "\n",
    "The data were compiled by students in this course: http://english197s2015.pbworks.com/w/page/93127947/FrontPage\n",
    "Found here: http://dhresourcesforprojectbuilding.pbworks.com/w/page/69244469/Data%20Collections%20and%20Datasets#demo-corpora\n",
    "\n",
    "That page has additional corpora, for those interested in exploring text analysis further.\n",
    "\n",
    "$^1$ Reference: Blei, D. M., A. Y. Ng, and M. I. Jordan (2003). Latent Dirichlet allocation. Journal of Machine\n",
    "Learning Research 3, 993–1022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author gender</th>\n",
       "      <th>year</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Dog with a Bad Name</td>\n",
       "      <td>Male</td>\n",
       "      <td>1886</td>\n",
       "      <td>A DOG WITH A BAD NAME  BY TALBOT BAINES REED  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Final Reckoning</td>\n",
       "      <td>Male</td>\n",
       "      <td>1887</td>\n",
       "      <td>A Final Reckoning: A Tale of Bush Life in Aust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A House Party, Don Gesualdo, and A Rainy June</td>\n",
       "      <td>Female</td>\n",
       "      <td>1887</td>\n",
       "      <td>A HOUSE-PARTY  Don Gesualdo  and  A Rainy June...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Houseful of Girls</td>\n",
       "      <td>Female</td>\n",
       "      <td>1889</td>\n",
       "      <td>A HOUSEFUL OF GIRLS. BY SARAH TYTLER,  AUTHOR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Little Country Girl</td>\n",
       "      <td>Female</td>\n",
       "      <td>1885</td>\n",
       "      <td>LITTLE COUNTRY GIRL.  BY  SUSAN COOLIDGE,     ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           title author gender  year  \\\n",
       "0                          A Dog with a Bad Name          Male  1886   \n",
       "1                              A Final Reckoning          Male  1887   \n",
       "2  A House Party, Don Gesualdo, and A Rainy June        Female  1887   \n",
       "3                            A Houseful of Girls        Female  1889   \n",
       "4                          A Little Country Girl        Female  1885   \n",
       "\n",
       "                                                text  \n",
       "0  A DOG WITH A BAD NAME  BY TALBOT BAINES REED  ...  \n",
       "1  A Final Reckoning: A Tale of Bush Life in Aust...  \n",
       "2  A HOUSE-PARTY  Don Gesualdo  and  A Rainy June...  \n",
       "3  A HOUSEFUL OF GIRLS. BY SARAH TYTLER,  AUTHOR ...  \n",
       "4  LITTLE COUNTRY GIRL.  BY  SUSAN COOLIDGE,     ...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "literature_fname = os.path.join(DATA_DIR, 'childrens_lit.csv.bz2')\n",
    "df_lit = pd.read_csv(literature_fname, sep='\\t', encoding = 'utf-8', compression = 'bz2', index_col=0)\n",
    "\n",
    "#drop rows where the text is missing\n",
    "df_lit = df_lit.dropna(subset=['text'])\n",
    "df_lit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to fit the model. This requires the use of CountVectorizer, which we've already used, and the scikit-learn function LatentDirichletAllocation.\n",
    "\n",
    "See [here](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) for more information about this function. \n",
    "\n",
    "First, we have to import it from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn, the input to LDA is a DTM (with either counts or TF-IDF scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.80, min_df=50,\n",
    "                                   stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(df_lit['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.80, min_df=50,\n",
    "                                stop_words='english'\n",
    "                                )\n",
    "tf = tf_vectorizer.fit_transform(df_lit['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "lda = LatentDirichletAllocation(n_components=3, max_iter=20, random_state=0)\n",
    "lda = lda.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function to print out the top words for each topic in a pretty way. Don't worry too much about understanding every line of this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #{}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "project doctor girls sister papa works mamma street london baby sweet dr remarked tea foundation youth office aunt ma presently\n",
      "\n",
      "Topic #1:\n",
      "dick jack uncle doctor er tom ain yer fish em den ye rock gun ha indian lads rope wolf ay\n",
      "\n",
      "Topic #2:\n",
      "king army french troops ship attack officers camp john soldiers officer prince city shore village guns rode frank regiment boats\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Modify the script above to:\n",
    "* increase the number of topics\n",
    "* increase the number of printed top words per topic\n",
    "* fit the model to the tf-idf matrix instead of the tf one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic weights\n",
    "\n",
    "One thing we may want to do with the output is compare the prevalence of each topic across documents. A simple way to do this (but not memory efficient), is to merge the topic distribution back into the Pandas dataframe.\n",
    "\n",
    "First get the topic distribution array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.49111434e-01, 5.47542026e-02, 9.61343630e-02],\n",
       "       [2.39026123e-01, 1.69669379e-01, 5.91304498e-01],\n",
       "       [9.19554017e-01, 1.46606825e-02, 6.57853008e-02],\n",
       "       [9.65945095e-01, 9.20765795e-05, 3.39628283e-02],\n",
       "       [8.66538007e-01, 8.40382943e-02, 4.94236986e-02],\n",
       "       [8.21693484e-01, 1.66616009e-01, 1.16905069e-02],\n",
       "       [4.28122865e-01, 5.71665020e-01, 2.12115021e-04],\n",
       "       [9.56618462e-01, 5.43789740e-05, 4.33271588e-02],\n",
       "       [2.86706649e-01, 4.26766910e-01, 2.86526441e-01],\n",
       "       [1.09886852e-04, 5.08130892e-01, 4.91759221e-01],\n",
       "       [1.17796850e-01, 2.24542559e-01, 6.57660590e-01],\n",
       "       [4.06208552e-01, 1.11933206e-01, 4.81858241e-01],\n",
       "       [4.90255047e-01, 7.76327414e-02, 4.32112212e-01],\n",
       "       [7.68568439e-01, 1.32904315e-01, 9.85272460e-02],\n",
       "       [6.08309166e-01, 1.46907419e-01, 2.44783415e-01],\n",
       "       [5.87879175e-01, 8.29725793e-02, 3.29148246e-01],\n",
       "       [5.74557968e-02, 8.49124305e-04, 9.41695079e-01],\n",
       "       [4.81146672e-01, 9.49447250e-02, 4.23908603e-01],\n",
       "       [9.21648931e-02, 9.07787797e-01, 4.73101320e-05],\n",
       "       [4.19603927e-05, 9.52153807e-01, 4.78042322e-02],\n",
       "       [6.72260649e-02, 1.37907810e-01, 7.94866125e-01],\n",
       "       [2.60896595e-01, 2.31080575e-01, 5.08022830e-01],\n",
       "       [9.12668792e-01, 3.65729827e-02, 5.07582249e-02],\n",
       "       [5.00842881e-02, 6.57372569e-01, 2.92543142e-01],\n",
       "       [2.17671532e-03, 9.97787660e-01, 3.56248980e-05],\n",
       "       [4.52346864e-01, 8.76307181e-02, 4.60022418e-01],\n",
       "       [9.13373182e-01, 4.04484567e-02, 4.61783613e-02],\n",
       "       [9.99875990e-01, 6.00441662e-05, 6.39659777e-05],\n",
       "       [9.99872412e-01, 6.45287394e-05, 6.30588137e-05],\n",
       "       [4.25982511e-01, 4.11035811e-01, 1.62981677e-01],\n",
       "       [8.20921442e-01, 1.59591956e-01, 1.94866025e-02],\n",
       "       [6.62380850e-01, 3.28462262e-01, 9.15688779e-03],\n",
       "       [2.10365739e-02, 5.38900575e-02, 9.25073369e-01],\n",
       "       [3.35727068e-05, 3.24906476e-05, 9.99933937e-01],\n",
       "       [6.11042551e-02, 4.14195013e-05, 9.38854325e-01],\n",
       "       [6.65420089e-01, 1.31787900e-03, 3.33262032e-01],\n",
       "       [9.94034750e-01, 7.07959769e-05, 5.89445399e-03],\n",
       "       [7.36832418e-01, 2.63058746e-01, 1.08835642e-04],\n",
       "       [4.71977178e-01, 3.57793291e-02, 4.92243493e-01],\n",
       "       [4.21059671e-01, 1.58822983e-02, 5.63058031e-01],\n",
       "       [5.41426120e-03, 1.47502122e-01, 8.47083617e-01],\n",
       "       [5.95138178e-03, 1.67762342e-01, 8.26286276e-01],\n",
       "       [6.83709724e-01, 7.98245076e-02, 2.36465769e-01],\n",
       "       [9.03052170e-01, 6.54890446e-02, 3.14587852e-02],\n",
       "       [8.75946022e-01, 8.51779951e-02, 3.88759825e-02],\n",
       "       [9.99429249e-01, 2.67340300e-04, 3.03410641e-04],\n",
       "       [3.38890756e-01, 3.99825585e-01, 2.61283659e-01],\n",
       "       [8.42429166e-01, 1.22943196e-01, 3.46276379e-02],\n",
       "       [3.83384525e-03, 9.80782823e-01, 1.53833322e-02],\n",
       "       [1.87991357e-02, 6.81277936e-01, 2.99922928e-01],\n",
       "       [3.86003351e-05, 8.86383245e-01, 1.13578154e-01],\n",
       "       [8.98178986e-01, 8.83924955e-02, 1.34285188e-02],\n",
       "       [7.78511275e-01, 2.18465392e-01, 3.02333232e-03],\n",
       "       [3.47875462e-03, 9.96483638e-01, 3.76072622e-05],\n",
       "       [2.40173574e-02, 4.35896267e-05, 9.75939053e-01],\n",
       "       [6.69251167e-02, 9.29842794e-01, 3.23208982e-03],\n",
       "       [9.11625304e-01, 7.24482948e-02, 1.59264013e-02],\n",
       "       [8.54105304e-01, 9.39476365e-02, 5.19470598e-02],\n",
       "       [9.95656390e-01, 4.28947043e-03, 5.41391325e-05],\n",
       "       [8.90739442e-01, 1.83531632e-02, 9.09073947e-02],\n",
       "       [5.26011635e-01, 2.21494808e-04, 4.73766871e-01],\n",
       "       [2.20379389e-01, 7.79578737e-01, 4.18742025e-05],\n",
       "       [7.41561964e-01, 1.11882084e-01, 1.46555951e-01],\n",
       "       [9.70676345e-01, 1.05998657e-02, 1.87237888e-02],\n",
       "       [8.65623235e-01, 1.28295919e-01, 6.08084647e-03],\n",
       "       [4.25582017e-01, 1.36568330e-01, 4.37849653e-01],\n",
       "       [3.49799778e-01, 6.49790525e-01, 4.09696605e-04],\n",
       "       [1.07868935e-02, 3.48001399e-05, 9.89178306e-01],\n",
       "       [4.76896121e-01, 2.13824741e-01, 3.09279138e-01],\n",
       "       [3.56858565e-01, 4.97870828e-03, 6.38162727e-01],\n",
       "       [1.98562249e-01, 5.23394835e-01, 2.78042916e-01],\n",
       "       [4.87776880e-01, 5.12125825e-01, 9.72949630e-05],\n",
       "       [5.91423265e-01, 1.70576977e-01, 2.37999758e-01],\n",
       "       [4.24944706e-01, 4.83174725e-02, 5.26737821e-01],\n",
       "       [6.19603632e-01, 1.98634449e-01, 1.81761919e-01],\n",
       "       [6.52040257e-01, 3.47677430e-01, 2.82313338e-04],\n",
       "       [5.81520498e-01, 3.17773952e-02, 3.86702106e-01],\n",
       "       [1.18294103e-02, 4.41406379e-02, 9.44029952e-01],\n",
       "       [1.48309219e-01, 1.50882523e-04, 8.51539898e-01],\n",
       "       [9.01790839e-01, 3.91221891e-02, 5.90869719e-02],\n",
       "       [1.08342701e-01, 4.10739553e-05, 8.91616225e-01],\n",
       "       [5.25076187e-01, 2.27013958e-01, 2.47909855e-01],\n",
       "       [4.18312070e-01, 3.55223683e-02, 5.46165561e-01],\n",
       "       [4.64155523e-05, 4.36483141e-05, 9.99909936e-01],\n",
       "       [6.14156592e-01, 2.62688175e-01, 1.23155232e-01],\n",
       "       [9.73658103e-01, 1.66474014e-02, 9.69449544e-03],\n",
       "       [6.19704556e-01, 4.55248711e-02, 3.34770573e-01],\n",
       "       [5.85031767e-01, 2.34548702e-01, 1.80419531e-01],\n",
       "       [4.24300657e-01, 3.96686838e-01, 1.79012504e-01],\n",
       "       [8.31380463e-01, 9.56618044e-02, 7.29577326e-02],\n",
       "       [6.75215685e-01, 1.16881700e-01, 2.07902615e-01],\n",
       "       [2.96596595e-01, 1.80579889e-01, 5.22823515e-01],\n",
       "       [6.37435196e-01, 5.67544454e-02, 3.05810359e-01],\n",
       "       [5.37840409e-02, 3.95735962e-05, 9.46176386e-01],\n",
       "       [2.19939191e-03, 3.10698441e-03, 9.94693624e-01],\n",
       "       [8.07462345e-01, 1.39236790e-01, 5.33008649e-02],\n",
       "       [7.09161839e-01, 1.62518622e-01, 1.28319540e-01],\n",
       "       [9.66623398e-01, 3.53649082e-03, 2.98401109e-02],\n",
       "       [7.89772456e-01, 5.07584966e-02, 1.59469048e-01],\n",
       "       [9.85851812e-02, 7.57791669e-01, 1.43623150e-01],\n",
       "       [9.99896895e-01, 4.92323416e-05, 5.38722079e-05],\n",
       "       [5.57373020e-01, 2.42251923e-01, 2.00375057e-01],\n",
       "       [4.12229861e-01, 2.34022084e-01, 3.53748055e-01],\n",
       "       [5.32277397e-01, 2.87422774e-01, 1.80299830e-01],\n",
       "       [4.56106257e-05, 8.22491768e-01, 1.77462621e-01],\n",
       "       [7.39596951e-01, 2.49131519e-01, 1.12715300e-02],\n",
       "       [6.72355760e-01, 1.15509369e-01, 2.12134872e-01],\n",
       "       [9.65395825e-01, 3.44734974e-02, 1.30677246e-04],\n",
       "       [9.97481161e-01, 2.45400936e-03, 6.48293157e-05],\n",
       "       [9.23082615e-01, 9.96462017e-03, 6.69527644e-02],\n",
       "       [6.08455409e-02, 1.23703065e-01, 8.15451394e-01],\n",
       "       [3.29440225e-03, 3.83546384e-03, 9.92870134e-01],\n",
       "       [5.02995054e-05, 1.97626715e-01, 8.02322985e-01],\n",
       "       [7.30256905e-01, 1.37151988e-01, 1.32591107e-01],\n",
       "       [2.15161322e-02, 9.53461097e-01, 2.50227709e-02],\n",
       "       [1.25062379e-04, 9.91021812e-01, 8.85312570e-03],\n",
       "       [6.78351625e-01, 8.63470061e-02, 2.35301369e-01],\n",
       "       [2.37052350e-01, 5.21180240e-01, 2.41767409e-01],\n",
       "       [5.07487011e-01, 3.43138297e-01, 1.49374692e-01],\n",
       "       [1.22515795e-01, 6.14805128e-01, 2.62679078e-01],\n",
       "       [8.91254267e-02, 9.10785510e-01, 8.90627982e-05],\n",
       "       [2.69088634e-03, 6.94989396e-02, 9.27810174e-01],\n",
       "       [3.81783743e-01, 4.36066648e-02, 5.74609592e-01],\n",
       "       [9.16103591e-01, 2.82275385e-03, 8.10736555e-02],\n",
       "       [2.74480493e-02, 4.26283545e-05, 9.72509322e-01],\n",
       "       [3.15148833e-05, 3.61359596e-02, 9.63832526e-01],\n",
       "       [4.98629943e-02, 5.29228751e-02, 8.97214131e-01]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_dist = lda.transform(tf)\n",
    "topic_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge back with original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>title</th>\n",
       "      <th>author gender</th>\n",
       "      <th>year</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.849111</td>\n",
       "      <td>0.054754</td>\n",
       "      <td>0.096134</td>\n",
       "      <td>A Dog with a Bad Name</td>\n",
       "      <td>Male</td>\n",
       "      <td>1886.0</td>\n",
       "      <td>A DOG WITH A BAD NAME  BY TALBOT BAINES REED  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.239026</td>\n",
       "      <td>0.169669</td>\n",
       "      <td>0.591304</td>\n",
       "      <td>A Final Reckoning</td>\n",
       "      <td>Male</td>\n",
       "      <td>1887.0</td>\n",
       "      <td>A Final Reckoning: A Tale of Bush Life in Aust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.919554</td>\n",
       "      <td>0.014661</td>\n",
       "      <td>0.065785</td>\n",
       "      <td>A House Party, Don Gesualdo, and A Rainy June</td>\n",
       "      <td>Female</td>\n",
       "      <td>1887.0</td>\n",
       "      <td>A HOUSE-PARTY  Don Gesualdo  and  A Rainy June...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.965945</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.033963</td>\n",
       "      <td>A Houseful of Girls</td>\n",
       "      <td>Female</td>\n",
       "      <td>1889.0</td>\n",
       "      <td>A HOUSEFUL OF GIRLS. BY SARAH TYTLER,  AUTHOR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.866538</td>\n",
       "      <td>0.084038</td>\n",
       "      <td>0.049424</td>\n",
       "      <td>A Little Country Girl</td>\n",
       "      <td>Female</td>\n",
       "      <td>1885.0</td>\n",
       "      <td>LITTLE COUNTRY GIRL.  BY  SUSAN COOLIDGE,     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.381784</td>\n",
       "      <td>0.043607</td>\n",
       "      <td>0.574610</td>\n",
       "      <td>Treasure Island</td>\n",
       "      <td>Male</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>TREASURE ISLAND  by Robert Louis Stevenson    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.916104</td>\n",
       "      <td>0.002823</td>\n",
       "      <td>0.081074</td>\n",
       "      <td>Twice Bought</td>\n",
       "      <td>Male</td>\n",
       "      <td>1885.0</td>\n",
       "      <td>The Project Gutenberg EBook of Twice Bought, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.027448</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.972509</td>\n",
       "      <td>Two Arrows</td>\n",
       "      <td>Male</td>\n",
       "      <td>1886.0</td>\n",
       "      <td>TWO ARROWS      HARPER'S YOUNG PEOPLE'S SERIES...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.036136</td>\n",
       "      <td>0.963833</td>\n",
       "      <td>Uncle Remus: His Songs and Sayings</td>\n",
       "      <td>Male</td>\n",
       "      <td>1880.0</td>\n",
       "      <td>Uncle Remus: His Songs and His Sayings  By Joe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.049863</td>\n",
       "      <td>0.052923</td>\n",
       "      <td>0.897214</td>\n",
       "      <td>Under Drake's Flag</td>\n",
       "      <td>Male</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>Under Drake's Flag:  A Tale of the Spanish Mai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2  \\\n",
       "0    0.849111  0.054754  0.096134   \n",
       "1    0.239026  0.169669  0.591304   \n",
       "2    0.919554  0.014661  0.065785   \n",
       "3    0.965945  0.000092  0.033963   \n",
       "4    0.866538  0.084038  0.049424   \n",
       "..        ...       ...       ...   \n",
       "122  0.381784  0.043607  0.574610   \n",
       "123  0.916104  0.002823  0.081074   \n",
       "124  0.027448  0.000043  0.972509   \n",
       "125  0.000032  0.036136  0.963833   \n",
       "126  0.049863  0.052923  0.897214   \n",
       "\n",
       "                                             title author gender    year  \\\n",
       "0                            A Dog with a Bad Name          Male  1886.0   \n",
       "1                                A Final Reckoning          Male  1887.0   \n",
       "2    A House Party, Don Gesualdo, and A Rainy June        Female  1887.0   \n",
       "3                              A Houseful of Girls        Female  1889.0   \n",
       "4                            A Little Country Girl        Female  1885.0   \n",
       "..                                             ...           ...     ...   \n",
       "122                                Treasure Island          Male  1883.0   \n",
       "123                                   Twice Bought          Male  1885.0   \n",
       "124                                     Two Arrows          Male  1886.0   \n",
       "125             Uncle Remus: His Songs and Sayings          Male  1880.0   \n",
       "126                             Under Drake's Flag          Male  1883.0   \n",
       "\n",
       "                                                  text  \n",
       "0    A DOG WITH A BAD NAME  BY TALBOT BAINES REED  ...  \n",
       "1    A Final Reckoning: A Tale of Bush Life in Aust...  \n",
       "2    A HOUSE-PARTY  Don Gesualdo  and  A Rainy June...  \n",
       "3    A HOUSEFUL OF GIRLS. BY SARAH TYTLER,  AUTHOR ...  \n",
       "4    LITTLE COUNTRY GIRL.  BY  SUSAN COOLIDGE,     ...  \n",
       "..                                                 ...  \n",
       "122  TREASURE ISLAND  by Robert Louis Stevenson    ...  \n",
       "123  The Project Gutenberg EBook of Twice Bought, b...  \n",
       "124  TWO ARROWS      HARPER'S YOUNG PEOPLE'S SERIES...  \n",
       "125  Uncle Remus: His Songs and His Sayings  By Joe...  \n",
       "126  Under Drake's Flag:  A Tale of the Spanish Mai...  \n",
       "\n",
       "[127 rows x 7 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_dist_df = pd.DataFrame(topic_dist)\n",
    "df_w_topics = topic_dist_df.join(df_lit)\n",
    "df_w_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can chech the average weight of each topic across gender using `groupby`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author gender\n",
       "Female    0.669019\n",
       "Male      0.407478\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = df_w_topics.groupby('author gender')\n",
    "grouped[0].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA as dimensionality reduction\n",
    "\n",
    "Now that we obtained a distribution of topic weights for each document, we can represent our corpus with a dense document-weight matrix as opposed to our initial sparse DTM. The weights can then replace tokens as features for any subsequent task (classification, prediction, etc). A simple example may consist in measuring cosine similarity between documents. For instance, which book is closest to the first book in our corpus? Let's use pairwise cosine similarity to find out. \n",
    "\n",
    "NB: cosine similarity measures an angle between two vectors, which provides a measure of distance robust to vectors of different lenghts (total number of tokens)\n",
    "\n",
    "First, let's turn the DTM into a readable dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = pd.DataFrame(tf_vectorizer.fit_transform(df_lit['text']).toarray(), columns=tf_vectorizer.get_feature_names(), index = df_lit.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's import the cosine_similarity function from sklearn and print the cosine similarity between the first and second book or the first and third book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 0.  1.  0. ... 23.  1. 14.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-da5fd04b0d9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cosine similarity between first and second book: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cosine similarity between first and third book: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1165\u001b[0m     \u001b[1;31m# to avoid recursive import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    139\u001b[0m         X = check_array(X, accept_sparse=accept_sparse, dtype=dtype,\n\u001b[0;32m    140\u001b[0m                         \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                         estimator=estimator)\n\u001b[0m\u001b[0;32m    142\u001b[0m         Y = check_array(Y, accept_sparse=accept_sparse, dtype=dtype,\n\u001b[0;32m    143\u001b[0m                         \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    554\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 0.  1.  0. ... 23.  1. 14.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(\"Cosine similarity between first and second book: \" + str(cosine_similarity(dtm.iloc[0,:], dtm.iloc[1,:])))\n",
    "print(\"Cosine similarity between first and third book: \" + str(cosine_similarity(dtm.iloc[0,:], dtm.iloc[2,:])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we use the topic weights instead of word frequencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dwm = df_w_topics.iloc[:,:10]\n",
    "\n",
    "print(\"Cosine similarity between first and second book: \" + str(cosine_similarity(dwm.iloc[0,:], dwm.iloc[1,:])))\n",
    "print(\"Cosine similarity between first and third book: \" + str(cosine_similarity(dwm.iloc[0,:], dwm.iloc[2,:])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Calculate the cosine similarity between the first book and all other books to identify the most similar one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further resources\n",
    "\n",
    "[This blog post](https://de.dariah.eu/tatom/feature_selection.html) goes through finding distinctive words using Python in more detail \n",
    "\n",
    "Paper: [Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf), Burt Monroe, Michael Colaresi, Kevin Quinn\n",
    "\n",
    "[Topic modeling with Textacy](https://github.com/repmax/topic-model/blob/master/topic-modelling.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
